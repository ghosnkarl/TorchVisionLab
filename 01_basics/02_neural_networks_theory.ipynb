{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# 02. Neural Networks Theory - A Beginner's Guide\n",
        "\n",
        "Welcome! üëã\n",
        "\n",
        "This notebook will help you understand **what neural networks are** and **how they actually work** using simple language, real-world examples, and clear explanations.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "By the end of this notebook, you'll understand:\n",
        "\n",
        "- What neural networks are (with everyday analogies)\n",
        "- The basic structure of a neural network\n",
        "- How information flows through a network (forward pass)\n",
        "- How networks measure their mistakes (loss)\n",
        "- How networks learn from their mistakes (gradients & backpropagation)\n",
        "- How networks improve their predictions (optimization)\n",
        "- The complete training cycle\n",
        "\n",
        "**No heavy math - just clear, intuitive explanations!** üéØ\n",
        "\n",
        "Think of this as building your mental model before we write any PyTorch code.\n",
        "\n",
        "> **Note:** This notebook focuses on **understanding the concepts**. In the next notebook, we'll build a real neural network in PyTorch and see these concepts in action!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "what_is_nn",
      "metadata": {},
      "source": [
        "## 1. What is a Neural Network?\n",
        "\n",
        "### The Simple Answer\n",
        "\n",
        "A neural network is a **computer program that learns from examples** to make predictions or decisions.\n",
        "\n",
        "Instead of you writing explicit rules, the network **figures out the patterns by itself** just by looking at data!\n",
        "\n",
        "### Real-World Analogy: The Restaurant Decision üçΩÔ∏è\n",
        "\n",
        "Imagine deciding if you'll like a restaurant:\n",
        "\n",
        "<video width=\"800\" controls autoplay muted loop>\n",
        "  <source src='../12_assets/restaurant_decision.mov' type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "</video>\n",
        "\n",
        "**You consider multiple factors:**\n",
        "\n",
        "- Food quality: 9/10 ‚≠ê\n",
        "- Price: $$$$ üí∞\n",
        "- Distance: 2 miles üìç\n",
        "- Reviews: 4.5/5 ‚≠ê\n",
        "\n",
        "**You weight them differently based on what matters to YOU:**\n",
        "\n",
        "- Food quality: Very important! (weight = 0.9)\n",
        "- Price: Somewhat important (weight = 0.6)\n",
        "- Distance: Less important (weight = 0.3)\n",
        "- Reviews: Important (weight = 0.8)\n",
        "\n",
        "**You combine them mentally and decide:**\n",
        "\n",
        "- \"Yes, I'll go!\" or \"No, not for me\"\n",
        "\n",
        "**A neural network works exactly this way:**\n",
        "\n",
        "- Takes multiple inputs (features)\n",
        "- Learns weights for each input (importance)\n",
        "- Combines them to make a decision (prediction)\n",
        "\n",
        "The key difference? **The network learns the best weights automatically from examples!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "network_structure",
      "metadata": {},
      "source": [
        "## 2. The Structure: Building Blocks of a Neural Network\n",
        "\n",
        "Every neural network has **three main parts** - think of them as layers in a sandwich! ü•™\n",
        "\n",
        "<img src='../12_assets/neural_network_building_blocks.png' alt='Neural Network Layers' width='800'/>\n",
        "\n",
        "### üî∑ Input Layer (The Starting Point)\n",
        "\n",
        "This is where your data enters the network.\n",
        "\n",
        "**Example: Predicting House Prices üè†**\n",
        "\n",
        "Your inputs might be:\n",
        "\n",
        "- Size: 2000 sq ft\n",
        "- Bedrooms: 3\n",
        "- Age: 10 years\n",
        "- Distance to city: 5 miles\n",
        "\n",
        "Each input gets its own spot (called a **neuron** or **node**).\n",
        "\n",
        "**Think of it as:** The reception desk where information enters the building.\n",
        "\n",
        "### üî∑ Hidden Layer(s) (The Thinking Part)\n",
        "\n",
        "This is where the \"magic\" happens! The network processes and transforms your inputs.\n",
        "\n",
        "**What happens here:**\n",
        "\n",
        "- Combines inputs in different ways\n",
        "- Finds patterns and relationships\n",
        "- Transforms data to make better predictions\n",
        "\n",
        "**You can have multiple hidden layers:**\n",
        "\n",
        "- 1 layer: Simple patterns\n",
        "- 2-3 layers: More complex patterns\n",
        "- Many layers: Very complex patterns (this is \"deep learning\"!)\n",
        "\n",
        "**Think of it as:** The offices where workers process and analyze the information.\n",
        "\n",
        "### üî∑ Output Layer (The Answer)\n",
        "\n",
        "This gives you the final prediction or decision.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "- House price prediction ‚Üí Output: $450,000\n",
        "- Email spam detection ‚Üí Output: Spam (1) or Not Spam (0)\n",
        "- Image classification ‚Üí Output: Cat, Dog, or Bird\n",
        "- Student pass/fail ‚Üí Output: 0.85 (85% chance of passing)\n",
        "\n",
        "**Think of it as:** The final report that leaves the building.\n",
        "\n",
        "### Key Concept: Connections\n",
        "\n",
        "**Every neuron in one layer connects to neurons in the next layer!**\n",
        "\n",
        "These connections are where the **learning happens**. Each connection has a **weight** (a number) that determines how important that connection is.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "how_predictions_work",
      "metadata": {},
      "source": [
        "## 3. The Forward Pass: How Networks Make Predictions\n",
        "\n",
        "The **forward pass** is when data flows through the network from input to output to make a prediction.\n",
        "\n",
        "Let's follow a complete example step by step!\n",
        "\n",
        "### Our Example: Will the Student Pass the Exam? üìö\n",
        "\n",
        "<video width=\"800\" controls autoplay muted loop>\n",
        "  <source src='../12_assets/forward_pass.mov' type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "</video>\n",
        "\n",
        "**Input data for one student:**\n",
        "\n",
        "- Hours studied: 10\n",
        "- Hours of sleep: 7\n",
        "- Previous test score: 75\n",
        "- Attendance %: 80\n",
        "\n",
        "**Goal:** Predict if they'll pass (output close to 1) or fail (output close to 0)\n",
        "\n",
        "### Step 1: Start at the Input Layer\n",
        "\n",
        "The network receives the data:\n",
        "\n",
        "```\n",
        "Input Neuron 1: 10  (hours studied)\n",
        "Input Neuron 2: 7   (sleep hours)\n",
        "Input Neuron 3: 75  (previous score)\n",
        "Input Neuron 4: 80  (attendance)\n",
        "```\n",
        "\n",
        "Each input gets its own neuron. Simple so far!\n",
        "\n",
        "### Step 2: Moving to the Hidden Layer (The Math Part!)\n",
        "\n",
        "Here's where the interesting stuff happens. For **each neuron in the hidden layer**, we do two things:\n",
        "\n",
        "#### Part A: Weighted Sum (Multiply and Add)\n",
        "\n",
        "Each connection between layers has a **weight** - a number the network learns.\n",
        "\n",
        "Let's focus on just ONE neuron in the hidden layer:\n",
        "\n",
        "```\n",
        "Weights for this neuron:\n",
        "- Weight 1: 0.5  (for hours studied)\n",
        "- Weight 2: 0.3  (for sleep hours)\n",
        "- Weight 3: 0.4  (for previous score)\n",
        "- Weight 4: 0.2  (for attendance)\n",
        "\n",
        "Calculation:\n",
        "(10 √ó 0.5) + (7 √ó 0.3) + (75 √ó 0.4) + (80 √ó 0.2)\n",
        "= 5 + 2.1 + 30 + 16\n",
        "= 53.1\n",
        "```\n",
        "\n",
        "**Plus a bias** (another number to learn):\n",
        "\n",
        "```\n",
        "53.1 + 5 = 58.1\n",
        "```\n",
        "\n",
        "**What's a bias?** Think of it as a \"starting point\" or \"threshold.\" It helps the network be more flexible.\n",
        "\n",
        "#### Part B: Activation Function (Adding Non-linearity)\n",
        "\n",
        "We take that 58.1 and pass it through an **activation function**.\n",
        "\n",
        "For now, just know it transforms the number. We'll explain why in the next section!\n",
        "\n",
        "```\n",
        "After activation: 58.1 ‚Üí (some transformed value)\n",
        "```\n",
        "\n",
        "#### The Key Point:\n",
        "\n",
        "**Every neuron in the hidden layer does this same process!**\n",
        "\n",
        "If you have 5 neurons in the hidden layer:\n",
        "\n",
        "- Each gets its own set of weights\n",
        "- Each does its own weighted sum\n",
        "- Each applies the activation function\n",
        "- Now you have 5 new numbers!\n",
        "\n",
        "### Step 3: Moving to the Output Layer\n",
        "\n",
        "The process repeats! The outputs from the hidden layer become the inputs to the output layer.\n",
        "\n",
        "The output layer does the same:\n",
        "\n",
        "1. Weighted sum of all hidden layer outputs\n",
        "2. Add bias\n",
        "3. Apply activation (sometimes)\n",
        "\n",
        "### Step 4: Final Prediction!\n",
        "\n",
        "You get your final output:\n",
        "\n",
        "```\n",
        "Output: 0.78\n",
        "```\n",
        "\n",
        "This means: **78% chance the student will pass!** ‚úÖ\n",
        "\n",
        "### The Big Picture\n",
        "\n",
        "**Forward Pass = Data flows forward through the network**\n",
        "\n",
        "```\n",
        "Input Data ‚Üí [Weights & Bias] ‚Üí Hidden Layer(s) ‚Üí [Weights & Bias] ‚Üí Output\n",
        "```\n",
        "\n",
        "At each step:\n",
        "\n",
        "1. **Multiply** inputs by weights\n",
        "2. **Add** them all up\n",
        "3. **Add** a bias\n",
        "4. **Apply** activation function\n",
        "5. Pass to next layer\n",
        "\n",
        "This happens in **milliseconds** on your computer!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "how_learning_works",
      "metadata": {},
      "source": [
        "## 4. Measuring Mistakes: The Loss Function üìè\n",
        "\n",
        "Now we know how networks make predictions (forward pass). But how do we know if the prediction is **good or bad**?\n",
        "\n",
        "<video width=\"800\" controls autoplay muted loop>\n",
        "  <source src='../12_assets/loss_function.mov' type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "</video>\n",
        "\n",
        "### The Concept: Measuring Error\n",
        "\n",
        "**Loss** (also called **error** or **cost**) is just a number that tells us **how wrong the prediction is**.\n",
        "\n",
        "- **High loss** = Very wrong prediction üò¢\n",
        "- **Low loss** = Good prediction! üéâ\n",
        "- **Zero loss** = Perfect prediction! (rare in real life)\n",
        "\n",
        "### A Simple Example\n",
        "\n",
        "**Scenario:** Predicting if a student will pass an exam.\n",
        "\n",
        "**Case 1: Bad Prediction**\n",
        "\n",
        "```\n",
        "Network prediction: 0.2  (20% chance of passing)\n",
        "Reality: 1.0             (Student actually passed!)\n",
        "Difference: 0.8          (VERY WRONG!)\n",
        "Loss: High ‚ö†Ô∏è\n",
        "```\n",
        "\n",
        "**Case 2: Good Prediction**\n",
        "\n",
        "```\n",
        "Network prediction: 0.95  (95% chance of passing)\n",
        "Reality: 1.0              (Student actually passed!)\n",
        "Difference: 0.05          (Almost perfect!)\n",
        "Loss: Low ‚úÖ\n",
        "```\n",
        "\n",
        "### How We Calculate Loss\n",
        "\n",
        "There are different formulas, but the idea is always the same:\n",
        "**Compare prediction to reality, measure the difference.**\n",
        "\n",
        "#### Common Loss Functions:\n",
        "\n",
        "**1. Mean Squared Error (MSE)** - For predicting numbers\n",
        "\n",
        "```\n",
        "Loss = (Prediction - Actual)¬≤\n",
        "\n",
        "Example:\n",
        "Predicted house price: $400,000\n",
        "Actual house price: $450,000\n",
        "Loss = ($400,000 - $450,000)¬≤ = $2,500,000,000\n",
        "```\n",
        "\n",
        "Squaring makes all errors positive and penalizes big mistakes more!\n",
        "\n",
        "**2. Binary Cross-Entropy** - For yes/no predictions\n",
        "\n",
        "```\n",
        "Used when output is 0 or 1 (pass/fail, spam/not spam)\n",
        "\n",
        "If student actually passed (1):\n",
        "- Predicting 0.9 ‚Üí Small loss ‚úÖ\n",
        "- Predicting 0.1 ‚Üí Large loss ‚ö†Ô∏è\n",
        "```\n",
        "\n",
        "**You don't need to memorize the formulas!** PyTorch will calculate loss for you. Just understand the concept.\n",
        "\n",
        "### Why Loss Matters\n",
        "\n",
        "Loss is **how the network knows it needs to improve!**\n",
        "\n",
        "Think of it like:\n",
        "\n",
        "- **Loss = Your test score** (but inverted - lower is better)\n",
        "- **High loss = Failed the test** ‚Üí Need to study more!\n",
        "- **Low loss = Aced the test** ‚Üí You're doing great!\n",
        "\n",
        "The network's goal during training:\n",
        "\n",
        "```\n",
        "START: High loss (bad predictions)\n",
        "   ‚Üì\n",
        "LEARNING...\n",
        "   ‚Üì\n",
        "END: Low loss (good predictions!)\n",
        "```\n",
        "\n",
        "### Multiple Examples at Once\n",
        "\n",
        "In practice, we calculate loss over **many examples** and take the average:\n",
        "\n",
        "```\n",
        "Example 1 loss: 0.8\n",
        "Example 2 loss: 0.3\n",
        "Example 3 loss: 0.5\n",
        "Example 4 loss: 0.2\n",
        "\n",
        "Average Loss: (0.8 + 0.3 + 0.5 + 0.2) / 4 = 0.45\n",
        "```\n",
        "\n",
        "This average tells us: \"On average, how wrong are we?\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "simple_activation",
      "metadata": {},
      "source": [
        "## 5. Activation Functions: Adding the Magic ‚ú®\n",
        "\n",
        "Remember in the forward pass when we said we \"apply an activation function\"? Let's understand why we need it!\n",
        "\n",
        "<video width=\"800\" controls autoplay muted loop>\n",
        "  <source src='../12_assets/activation_functions.mp4' type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "</video>\n",
        "\n",
        "### The Problem Without Activation Functions\n",
        "\n",
        "Imagine a network that only multiplies and adds:\n",
        "\n",
        "```\n",
        "Neuron calculation: (Input1 √ó Weight1) + (Input2 √ó Weight2) + Bias\n",
        "```\n",
        "\n",
        "**The issue:** No matter how many layers you stack, this can only learn **straight lines**!\n",
        "\n",
        "**Real-world problems aren't straight lines:**\n",
        "\n",
        "- Is this email spam? (Not a straight line relationship)\n",
        "- Will it rain tomorrow? (Very complex patterns)\n",
        "- What's in this image? (Incredibly complex!)\n",
        "\n",
        "We need the network to learn **curves, boundaries, and complex patterns**.\n",
        "\n",
        "### The Solution: Activation Functions\n",
        "\n",
        "An activation function is just a **simple transformation** we apply after the weighted sum.\n",
        "\n",
        "**Instead of:**\n",
        "\n",
        "```\n",
        "Output = (weighted sum) + bias\n",
        "```\n",
        "\n",
        "**We do:**\n",
        "\n",
        "```\n",
        "Calculation = (weighted sum) + bias\n",
        "Output = activation_function(Calculation)\n",
        "```\n",
        "\n",
        "This simple extra step allows networks to learn **any pattern**, no matter how complex!\n",
        "\n",
        "### Meet ReLU: The Most Popular Activation üåü\n",
        "\n",
        "**ReLU** stands for \"Rectified Linear Unit\" - fancy name, super simple concept!\n",
        "\n",
        "**The rule:**\n",
        "\n",
        "```\n",
        "If the number is positive ‚Üí Keep it as is\n",
        "If the number is negative ‚Üí Make it zero\n",
        "```\n",
        "\n",
        "That's it! Really!\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "- Input: 58.1 ‚Üí Output: 58.1 ‚úÖ (positive, keep it)\n",
        "- Input: 5 ‚Üí Output: 5 ‚úÖ (positive, keep it)\n",
        "- Input: -3 ‚Üí Output: 0 ‚úÖ (negative, make it zero)\n",
        "- Input: -100 ‚Üí Output: 0 ‚úÖ (negative, make it zero)\n",
        "\n",
        "### Why Does This Help?\n",
        "\n",
        "This tiny \"if-then\" rule:\n",
        "\n",
        "- ‚úÖ Allows networks to learn curves and complex boundaries\n",
        "- ‚úÖ Makes networks much more powerful\n",
        "- ‚úÖ Is fast to compute\n",
        "- ‚úÖ Works really well in practice!\n",
        "\n",
        "### Where Do We Use It?\n",
        "\n",
        "**In hidden layers:** We apply ReLU (or another activation) after each hidden layer calculation.\n",
        "\n",
        "**In output layer:** We might use a different activation depending on the task:\n",
        "\n",
        "- For probabilities (0 to 1): Sigmoid activation\n",
        "- For multiple classes: Softmax activation\n",
        "- For regression (any number): Sometimes no activation!\n",
        "\n",
        "**Don't worry about these yet!** ReLU in hidden layers is what you need to know now.\n",
        "\n",
        "### The Intuition\n",
        "\n",
        "Think of ReLU as a **filter**:\n",
        "\n",
        "- Keeps positive signals (useful information)\n",
        "- Blocks negative signals (sets them to zero)\n",
        "\n",
        "This selective blocking and passing creates the complexity needed for learning!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "why_powerful",
      "metadata": {},
      "source": [
        "## 6. Learning from Mistakes: Gradients & Backpropagation üß†\n",
        "\n",
        "Now for the **most important part**: How does the network actually learn and improve?\n",
        "\n",
        "<video width=\"800\" controls autoplay muted loop>\n",
        "  <source src='../12_assets/gradients_backpropagation.mp4' type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "</video>\n",
        "\n",
        "### The Big Question\n",
        "\n",
        "We know:\n",
        "\n",
        "- The network made a prediction\n",
        "- We calculated the loss (error)\n",
        "- The loss is high (prediction was wrong)\n",
        "\n",
        "**Now what?** Which weights should we change? By how much?\n",
        "\n",
        "### The Intuition: Cause and Effect\n",
        "\n",
        "Imagine you're playing darts üéØ:\n",
        "\n",
        "1. You throw a dart\n",
        "2. It lands too far to the left\n",
        "3. **You need to adjust** ‚Üí throw more to the right next time\n",
        "\n",
        "**The key question:** How much more to the right?\n",
        "\n",
        "The network faces the same question:\n",
        "\n",
        "- The prediction was wrong\n",
        "- **Which weights caused the error?**\n",
        "- **How much should we change each weight?**\n",
        "\n",
        "### Meet Gradients: The Blame Assignment\n",
        "\n",
        "A **gradient** is a number that tells us:\n",
        "\n",
        "1. **Which direction to change a weight** (increase or decrease?)\n",
        "2. **How much impact that weight had on the error** (big change or small change?)\n",
        "\n",
        "**Think of it as:** \"How much is this weight to blame for the mistake?\"\n",
        "\n",
        "### A Simple Analogy: Hiking Down a Mountain üèîÔ∏è\n",
        "\n",
        "Imagine you're on a foggy mountain and want to go down:\n",
        "\n",
        "- You can't see the bottom (the goal)\n",
        "- But you can feel the slope under your feet\n",
        "- **You take small steps in the direction that goes down most**\n",
        "\n",
        "**Gradients are like feeling the slope:**\n",
        "\n",
        "- Positive gradient ‚Üí Weight should go DOWN\n",
        "- Negative gradient ‚Üí Weight should go UP\n",
        "- Large gradient ‚Üí This weight matters a lot!\n",
        "- Small gradient ‚Üí This weight matters less\n",
        "\n",
        "### Backpropagation: Working Backwards\n",
        "\n",
        "**\"Backpropagation\"** sounds scary, but it's just this idea:\n",
        "\n",
        "**We work backwards through the network** to figure out each weight's gradient.\n",
        "\n",
        "```\n",
        "Forward Pass:  Input ‚Üí Hidden ‚Üí Output ‚Üí Loss\n",
        "                    ‚Üí  ‚Üí  ‚Üí\n",
        "\n",
        "Backward Pass: Input ‚Üê Hidden ‚Üê Output ‚Üê Loss\n",
        "                    ‚Üê  ‚Üê  ‚Üê\n",
        "              (Calculate gradients)\n",
        "```\n",
        "\n",
        "### The Process Step-by-Step\n",
        "\n",
        "**Step 1:** Calculate loss (we already did this!)\n",
        "\n",
        "```\n",
        "Loss = 0.8  (high error!)\n",
        "```\n",
        "\n",
        "**Step 2:** Start at the output and ask:\n",
        "\n",
        "```\n",
        "\"How much did the output layer weights contribute to this error?\"\n",
        "Calculate gradients for output layer weights.\n",
        "```\n",
        "\n",
        "**Step 3:** Move back to hidden layer and ask:\n",
        "\n",
        "```\n",
        "\"How much did the hidden layer weights contribute to this error?\"\n",
        "Calculate gradients for hidden layer weights.\n",
        "```\n",
        "\n",
        "**Step 4:** Continue backwards through all layers!\n",
        "\n",
        "### The Math (Simple Version)\n",
        "\n",
        "**You don't need to do this manually!** But here's the concept:\n",
        "\n",
        "For each weight, we calculate:\n",
        "\n",
        "```\n",
        "Gradient = How much does loss change when we change this weight slightly?\n",
        "```\n",
        "\n",
        "This tells us:\n",
        "\n",
        "- If gradient is +2.5 ‚Üí Loss increases when weight increases (so reduce weight!)\n",
        "- If gradient is -1.8 ‚Üí Loss decreases when weight increases (so increase weight!)\n",
        "- If gradient is close to 0 ‚Üí This weight doesn't matter much for this example\n",
        "\n",
        "### The Good News! üéâ\n",
        "\n",
        "**PyTorch does ALL of this automatically!**\n",
        "\n",
        "You just call:\n",
        "\n",
        "```python\n",
        "loss.backward()  # PyTorch calculates all gradients!\n",
        "```\n",
        "\n",
        "PyTorch:\n",
        "\n",
        "1. Works backwards through the network\n",
        "2. Calculates the gradient for every single weight\n",
        "3. Stores them so we can use them to update the weights\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "‚úÖ **Gradients tell us how to change weights to reduce loss**\n",
        "‚úÖ **Backpropagation is just working backwards to calculate gradients**\n",
        "‚úÖ **Large gradient = this weight needs a big adjustment**\n",
        "‚úÖ **Small gradient = this weight is already pretty good**\n",
        "‚úÖ **PyTorch does the hard math for you automatically!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c519b61",
      "metadata": {},
      "source": [
        "## 7. Making It Better: The Optimizer üîß\n",
        "\n",
        "We now have gradients that tell us how to change each weight. But **how exactly do we update the weights?**\n",
        "\n",
        "This is where the **optimizer** comes in!\n",
        "\n",
        "### What is an Optimizer?\n",
        "\n",
        "An optimizer is the algorithm that **actually updates the weights** to make the network better.\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "- **Gradient** = The compass that points the direction\n",
        "- **Optimizer** = The hiking boots that take you there!\n",
        "\n",
        "### The Basic Update Rule\n",
        "\n",
        "The simplest form of weight update:\n",
        "\n",
        "```\n",
        "New Weight = Old Weight - (Learning Rate √ó Gradient)\n",
        "```\n",
        "\n",
        "Let's break this down:\n",
        "\n",
        "#### Part 1: The Gradient\n",
        "\n",
        "We just calculated this! It tells us:\n",
        "\n",
        "- If gradient is positive ‚Üí Weight should go DOWN\n",
        "- If gradient is negative ‚Üí Weight should go UP\n",
        "\n",
        "#### Part 2: The Learning Rate\n",
        "\n",
        "**Learning rate** is a small number (like 0.01 or 0.001) that controls **how big of a step we take**.\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "- **Large learning rate (0.1)** = Taking big steps ‚Üí Faster but might overshoot!\n",
        "- **Small learning rate (0.001)** = Taking tiny steps ‚Üí Slower but more precise\n",
        "\n",
        "**Typical learning rates:** 0.01, 0.001, 0.0001\n",
        "\n",
        "### Example: Updating One Weight\n",
        "\n",
        "Let's update a single weight:\n",
        "\n",
        "```\n",
        "Current weight: 0.5\n",
        "Gradient: 2.0 (positive ‚Üí weight should decrease)\n",
        "Learning rate: 0.01\n",
        "\n",
        "New weight = 0.5 - (0.01 √ó 2.0)\n",
        "           = 0.5 - 0.02\n",
        "           = 0.48 ‚úÖ\n",
        "```\n",
        "\n",
        "**What happened:** Weight decreased slightly, which will reduce the loss!\n",
        "\n",
        "### Another Example:\n",
        "\n",
        "```\n",
        "Current weight: 0.3\n",
        "Gradient: -1.5 (negative ‚Üí weight should increase)\n",
        "Learning rate: 0.01\n",
        "\n",
        "New weight = 0.3 - (0.01 √ó -1.5)\n",
        "           = 0.3 - (-0.015)\n",
        "           = 0.3 + 0.015\n",
        "           = 0.315 ‚úÖ\n",
        "```\n",
        "\n",
        "**What happened:** Weight increased slightly, which will reduce the loss!\n",
        "\n",
        "### The Most Common Optimizer: SGD (Stochastic Gradient Descent)\n",
        "\n",
        "\"Stochastic Gradient Descent\" sounds complex, but it's just the basic update rule we showed above!\n",
        "\n",
        "**\"Stochastic\"** means we update weights after seeing just a few examples (a \"batch\") instead of all the data.\n",
        "\n",
        "**\"Gradient Descent\"** means we're descending (going down) the mountain of loss using gradients.\n",
        "\n",
        "### In PyTorch\n",
        "\n",
        "Here's how easy it is:\n",
        "\n",
        "```python\n",
        "# Create an optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# After calculating loss and gradients...\n",
        "optimizer.step()  # Updates all weights automatically!\n",
        "```\n",
        "\n",
        "That's it! The optimizer updates every single weight in your network with one line!\n",
        "\n",
        "### The Optimizer's Job Summary\n",
        "\n",
        "1. ‚úÖ Take all the gradients (calculated by backpropagation)\n",
        "2. ‚úÖ Apply the learning rate\n",
        "3. ‚úÖ Update every weight in the network\n",
        "4. ‚úÖ Do this thousands of times until loss is low!\n",
        "\n",
        "### Other Optimizers (Don't Worry About These Yet!)\n",
        "\n",
        "There are fancier optimizers like:\n",
        "\n",
        "- **Adam** - Very popular, adapts learning rate automatically\n",
        "- **RMSprop** - Good for certain types of problems\n",
        "- **AdaGrad** - Adjusts learning rate per parameter\n",
        "\n",
        "**For now:** Just know SGD exists. We'll explore others later!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e733954b",
      "metadata": {},
      "source": [
        "## 8. Putting It All Together: The Training Loop üîÑ\n",
        "\n",
        "Now let's see how **all these pieces work together** to train a neural network!\n",
        "\n",
        "<video width=\"800\" controls autoplay muted loop>\n",
        "  <source src='../12_assets/putting_it_together.mp4' type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "</video>\n",
        "\n",
        "### The Complete Training Cycle\n",
        "\n",
        "Training a neural network is repeating these steps over and over:\n",
        "\n",
        "```\n",
        "1. FORWARD PASS    ‚Üí Make predictions\n",
        "2. CALCULATE LOSS  ‚Üí Measure how wrong we are\n",
        "3. BACKWARD PASS   ‚Üí Calculate gradients\n",
        "4. UPDATE WEIGHTS  ‚Üí Improve the network\n",
        "5. REPEAT!\n",
        "```\n",
        "\n",
        "Let's go through one complete cycle:\n",
        "\n",
        "---\n",
        "\n",
        "### üìç Starting Point\n",
        "\n",
        "```\n",
        "Network weights: Random values (network knows nothing yet!)\n",
        "Training data: 1000 student examples with features and pass/fail labels\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: FORWARD PASS üéØ\n",
        "\n",
        "**What happens:**\n",
        "\n",
        "- Take one example (or a small batch of examples)\n",
        "- Feed it through the network\n",
        "- Get a prediction\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```\n",
        "Input: [10 hours studied, 7 sleep, 75 prev_score, 80 attendance]\n",
        "        ‚Üì\n",
        "   [Network processing]\n",
        "        ‚Üì\n",
        "Output: 0.3 (30% chance of passing)\n",
        "```\n",
        "\n",
        "**In PyTorch:**\n",
        "\n",
        "```python\n",
        "predictions = model(input_data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: CALCULATE LOSS üìè\n",
        "\n",
        "**What happens:**\n",
        "\n",
        "- Compare prediction to the true answer\n",
        "- Calculate how wrong we are\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```\n",
        "Prediction: 0.3\n",
        "True label: 1.0 (student actually passed!)\n",
        "Loss: 0.8 (high error - very wrong!)\n",
        "```\n",
        "\n",
        "**In PyTorch:**\n",
        "\n",
        "```python\n",
        "loss = loss_function(predictions, true_labels)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: BACKWARD PASS (Backpropagation) ‚¨ÖÔ∏è\n",
        "\n",
        "**What happens:**\n",
        "\n",
        "- Calculate gradients for all weights\n",
        "- Figure out which weights to blame for the error\n",
        "- Determine how much to change each weight\n",
        "\n",
        "**In PyTorch:**\n",
        "\n",
        "```python\n",
        "loss.backward()  # Calculates all gradients automatically!\n",
        "```\n",
        "\n",
        "**Behind the scenes:**\n",
        "\n",
        "```\n",
        "Gradient for weight1: 2.5  (needs to decrease)\n",
        "Gradient for weight2: -1.2 (needs to increase)\n",
        "Gradient for weight3: 0.1  (barely needs to change)\n",
        "... (and so on for all weights)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: UPDATE WEIGHTS (Optimization) üîß\n",
        "\n",
        "**What happens:**\n",
        "\n",
        "- Use the gradients to update each weight\n",
        "- Make the network slightly better\n",
        "\n",
        "**In PyTorch:**\n",
        "\n",
        "```python\n",
        "optimizer.step()  # Updates all weights!\n",
        "```\n",
        "\n",
        "**Behind the scenes:**\n",
        "\n",
        "```\n",
        "weight1: 0.5 ‚Üí 0.475  (decreased based on gradient)\n",
        "weight2: 0.3 ‚Üí 0.312  (increased based on gradient)\n",
        "weight3: 0.8 ‚Üí 0.799  (tiny change)\n",
        "... (all weights updated!)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Step 5: ONE MORE IMPORTANT THING - Zero the Gradients! üßπ\n",
        "\n",
        "Before the next iteration, we need to clear the old gradients:\n",
        "\n",
        "**In PyTorch:**\n",
        "\n",
        "```python\n",
        "optimizer.zero_grad()  # Clear old gradients\n",
        "```\n",
        "\n",
        "**Why?** PyTorch accumulates gradients by default. We need to reset them for each new batch!\n",
        "\n",
        "---\n",
        "\n",
        "### The Complete Loop in Code\n",
        "\n",
        "Here's what one training cycle looks like in PyTorch:\n",
        "\n",
        "```python\n",
        "# One iteration of training\n",
        "for inputs, labels in training_data:\n",
        "\n",
        "    # 1. Forward pass\n",
        "    predictions = model(inputs)\n",
        "\n",
        "    # 2. Calculate loss\n",
        "    loss = loss_function(predictions, labels)\n",
        "\n",
        "    # 3. Zero gradients from previous iteration\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Backward pass (calculate gradients)\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Update weights\n",
        "    optimizer.step()\n",
        "```\n",
        "\n",
        "**That's it!** These ~10 lines train your neural network!\n",
        "\n",
        "---\n",
        "\n",
        "### What Happens Over Many Iterations?\n",
        "\n",
        "```\n",
        "Iteration 1:  Loss = 0.95  (terrible!)\n",
        "Iteration 10: Loss = 0.82  (still bad)\n",
        "Iteration 50: Loss = 0.45  (getting better!)\n",
        "Iteration 100: Loss = 0.22 (much better!)\n",
        "Iteration 500: Loss = 0.08 (pretty good!)\n",
        "```\n",
        "\n",
        "**The network gradually learns!** üéâ\n",
        "\n",
        "---\n",
        "\n",
        "### Key Terms to Remember\n",
        "\n",
        "- **Iteration/Step:** One complete cycle through the loop\n",
        "- **Batch:** A small group of examples processed together\n",
        "- **Epoch:** One complete pass through ALL your training data\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- You have 1000 training examples\n",
        "- Your batch size is 100\n",
        "- One epoch = 10 iterations (1000 √∑ 100)\n",
        "- Training for 50 epochs = 500 total iterations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## 9. Summary: Everything You Learned! üéâ\n",
        "\n",
        "Congratulations! You now understand the fundamental concepts of neural networks. Let's recap!\n",
        "\n",
        "---\n",
        "\n",
        "### üß† The Core Concepts\n",
        "\n",
        "#### 1. What Neural Networks Are\n",
        "\n",
        "- **Computer programs that learn from examples**\n",
        "- No need to write explicit rules - they figure out patterns automatically\n",
        "- Work like humans learning: see examples ‚Üí find patterns ‚Üí make predictions ‚Üí learn from mistakes\n",
        "\n",
        "#### 2. The Structure (3 Main Parts)\n",
        "\n",
        "- **Input Layer:** Where data enters (one neuron per feature)\n",
        "- **Hidden Layer(s):** Where processing and pattern recognition happens\n",
        "- **Output Layer:** Where the final prediction comes out\n",
        "\n",
        "#### 3. The Forward Pass (Making Predictions)\n",
        "\n",
        "```\n",
        "For each neuron:\n",
        "1. Multiply inputs by weights\n",
        "2. Add them up\n",
        "3. Add a bias\n",
        "4. Apply activation function (like ReLU)\n",
        "5. Pass to next layer\n",
        "\n",
        "Result: A prediction!\n",
        "```\n",
        "\n",
        "#### 4. Activation Functions (Why We Need Them)\n",
        "\n",
        "- **Without them:** Can only learn straight lines (limited!)\n",
        "- **With them:** Can learn any complex pattern\n",
        "- **ReLU:** Most common - keeps positive values, zeros negative values\n",
        "- **Makes networks powerful!**\n",
        "\n",
        "#### 5. Loss Function (Measuring Mistakes)\n",
        "\n",
        "- **Loss = how wrong the prediction is**\n",
        "- High loss = bad prediction üò¢\n",
        "- Low loss = good prediction! üéâ\n",
        "- Network's goal: minimize loss\n",
        "\n",
        "#### 6. Gradients & Backpropagation (Understanding Mistakes)\n",
        "\n",
        "- **Gradient:** Tells us how to change each weight to reduce loss\n",
        "- **Backpropagation:** Working backwards through the network to calculate gradients\n",
        "- Answers: \"Which weights caused the error? How much should they change?\"\n",
        "- **PyTorch does this automatically with `loss.backward()`**\n",
        "\n",
        "#### 7. Optimizer (Fixing Mistakes)\n",
        "\n",
        "- **Updates all weights to make network better**\n",
        "- Uses gradients and learning rate\n",
        "- Formula: `New Weight = Old Weight - (Learning Rate √ó Gradient)`\n",
        "- **PyTorch does this automatically with `optimizer.step()`**\n",
        "\n",
        "#### 8. The Training Loop (Putting It All Together)\n",
        "\n",
        "```python\n",
        "for data in training_data:\n",
        "    predictions = model(data)      # 1. Forward pass\n",
        "    loss = loss_fn(predictions)    # 2. Calculate loss\n",
        "    optimizer.zero_grad()          # 3. Clear old gradients\n",
        "    loss.backward()                # 4. Calculate new gradients\n",
        "    optimizer.step()               # 5. Update weights\n",
        "```\n",
        "\n",
        "**Repeat thousands of times ‚Üí Network learns!**\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ The Big Picture\n",
        "\n",
        "```\n",
        "Random Weights (Bad) ‚Üí Training Loop (Learning) ‚Üí Good Weights (Accurate!)\n",
        "                           ‚Üì\n",
        "              Forward ‚Üí Loss ‚Üí Backward ‚Üí Update\n",
        "                    (Repeat many times)\n",
        "```\n",
        "\n",
        "**Each iteration makes the network slightly better!**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Key Takeaways\n",
        "\n",
        "1. **Neural networks learn automatically from examples** - you don't write rules\n",
        "2. **Forward pass**: Data flows through to make predictions\n",
        "3. **Loss**: Measures how wrong predictions are\n",
        "4. **Backward pass**: Calculates how to improve\n",
        "5. **Optimizer**: Actually makes the improvements\n",
        "6. **Training loop**: Repeats this cycle until network is good\n",
        "7. **PyTorch does the hard math for you!**\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ What's Next?\n",
        "\n",
        "Now that you understand **how neural networks work**, you're ready to:\n",
        "\n",
        "1. **Build your first neural network in PyTorch!** üëâ Next notebook: `02b_neural_networks_intro.ipynb`\n",
        "2. See these concepts in actual code\n",
        "3. Train a network on real data\n",
        "4. Watch it learn and improve!\n",
        "\n",
        "---\n",
        "\n",
        "### üí™ You're Ready!\n",
        "\n",
        "You now have the mental model needed to understand PyTorch code. Don't worry if everything isn't 100% clear yet - **doing is the best way to learn!**\n",
        "\n",
        "The key is:\n",
        "\n",
        "- ‚úÖ You understand the big picture\n",
        "- ‚úÖ You know what each piece does\n",
        "- ‚úÖ You're ready to write code\n",
        "\n",
        "**Let's build something!** üéâ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "102909e7",
      "metadata": {},
      "source": [
        "## üìö Want to Learn More?\n",
        "\n",
        "Here are some great videos and articles to deepen your understanding of neural networks!\n",
        "\n",
        "---\n",
        "\n",
        "### üé• **Must-Watch Videos**\n",
        "\n",
        "#### **3Blue1Brown - Neural Networks Series**\n",
        "\n",
        "The best visual introduction to neural networks! Watch all 4 videos:\n",
        "\n",
        "1. **[But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk)** (19 min)\n",
        "\n",
        "   - Beautiful animations explaining how neural networks work\n",
        "   - Perfect starting point!\n",
        "\n",
        "2. **[Gradient descent, how neural networks learn](https://www.youtube.com/watch?v=IHZwWFHWa-w)** (21 min)\n",
        "\n",
        "   - Visualizes how networks learn from data\n",
        "   - Makes calculus intuitive\n",
        "\n",
        "3. **[What is backpropagation really doing?](https://www.youtube.com/watch?v=Ilg3gGewQ5U)** (14 min)\n",
        "\n",
        "   - Deep dive into how gradients flow backward\n",
        "   - Clear explanations of the math\n",
        "\n",
        "4. **[Backpropagation calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8)** (10 min)\n",
        "   - The mathematical details\n",
        "   - Optional if you want the full picture\n",
        "\n",
        "#### **StatQuest with Josh Starmer**\n",
        "\n",
        "Friendly, step-by-step explanations:\n",
        "\n",
        "- **[Neural Networks Part 1: Inside the Black Box](https://www.youtube.com/watch?v=CqOfi41LfDw)** (12 min)\n",
        "- **[Neural Networks Part 2: Backpropagation Main Ideas](https://www.youtube.com/watch?v=IN2XmBhILt4)** (8 min)\n",
        "\n",
        "#### **Andrej Karpathy**\n",
        "\n",
        "From a leading AI researcher:\n",
        "\n",
        "- **[The spelled-out intro to neural networks and backpropagation](https://www.youtube.com/watch?v=VMj-3S1tku0)** (2.5 hours)\n",
        "  - Build a neural network from scratch\n",
        "  - Understand every line of code\n",
        "\n",
        "---\n",
        "\n",
        "### üåê **Interactive Tools**\n",
        "\n",
        "Play with neural networks in your browser!\n",
        "\n",
        "1. **[TensorFlow Playground](https://playground.tensorflow.org/)**\n",
        "\n",
        "   - Experiment with different architectures\n",
        "   - See how networks learn in real-time\n",
        "   - Change activation functions, layers, and watch what happens\n",
        "\n",
        "2. **[Neural Network Visualizer](https://www.cs.ryerson.ca/~aharley/neural-networks/)**\n",
        "   - Click on neurons to see what they detect\n",
        "   - Multiple examples (handwritten digits, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### üìù **Great Articles**\n",
        "\n",
        "#### **Christopher Olah's Blog** - [colah.github.io](https://colah.github.io/)\n",
        "\n",
        "Crystal-clear explanations with beautiful visuals:\n",
        "\n",
        "- \"Neural Networks, Manifolds, and Topology\"\n",
        "- \"Understanding LSTM Networks\"\n",
        "- \"Visualizing Representations\"\n",
        "\n",
        "#### **Jay Alammar's Visual Guides** - [jalammar.github.io](https://jalammar.github.io/)\n",
        "\n",
        "Visual, intuitive explanations:\n",
        "\n",
        "- \"A Visual Guide to Neural Networks\"\n",
        "- \"Visual intro to machine learning\"\n",
        "\n",
        "#### **Distill.pub** - [distill.pub](https://distill.pub/)\n",
        "\n",
        "Interactive research articles:\n",
        "\n",
        "- Beautiful visualizations\n",
        "- High-quality explanations\n",
        "- Explore \"Feature Visualization\" and \"Activation Atlas\"\n",
        "\n",
        "---\n",
        "\n",
        "### üìñ **Free Online Book**\n",
        "\n",
        "**[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)** by Michael Nielsen\n",
        "\n",
        "- Free, comprehensive, and beginner-friendly\n",
        "- Interactive code examples\n",
        "- Clear mathematical explanations\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Suggested Learning Path**\n",
        "\n",
        "1. **Start here (1-2 hours):** Watch the 3Blue1Brown series\n",
        "2. **Play around (30 min):** Experiment with TensorFlow Playground\n",
        "3. **Go deeper (optional):** Watch StatQuest or Andrej Karpathy videos\n",
        "4. **Read more (optional):** Explore Christopher Olah's blog or Michael Nielsen's book\n",
        "\n",
        "---\n",
        "\n",
        "_Remember: The best way to learn is by doing! Try implementing what you watch and read._ üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
